{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s — %(levelname)s — %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Detectar GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# IterableDataset que lee shards en streaming\n",
    "class ParquetShardsDataset(IterableDataset):\n",
    "    def __init__(self, shards, batch_size, scaler=None):\n",
    "        \"\"\"\n",
    "        shards: lista de rutas a archivos parquet\n",
    "        batch_size: tamaño de lote\n",
    "        scaler: StandardScaler entrenado (solo para val)\n",
    "        \"\"\"\n",
    "        self.shards = shards\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __iter__(self):\n",
    "        for shard_path in self.shards:\n",
    "            df = pd.read_parquet(shard_path)\n",
    "            X, y = prepare_features_target(df)\n",
    "            if self.scaler is not None:\n",
    "                X = self.scaler.transform(X)\n",
    "            # emitir en batches\n",
    "            for i in range(0, len(y), self.batch_size):\n",
    "                Xb = X[i : i + self.batch_size]\n",
    "                yb = y[i : i + self.batch_size]\n",
    "                # convertir a tensores\n",
    "                Xb_t = torch.from_numpy(Xb).float()\n",
    "                yb_t = torch.from_numpy(yb.reshape(-1,1)).float()\n",
    "                yield Xb_t, yb_t\n",
    "\n",
    "# Definición de la red\n",
    "class PriceNet(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_with_shards(\n",
    "    train_dir, val_dir,\n",
    "    batch_size=1024, lr=1e-3, n_epochs=10\n",
    "):\n",
    "    # Listar shards\n",
    "    train_shards = sorted(glob.glob(os.path.join(train_dir, \"*.parquet\")))\n",
    "    val_shards   = sorted(glob.glob(os.path.join(val_dir,   \"*.parquet\")))\n",
    "    logger.info(f\"{len(train_shards)} shards de entrenamiento, \"\n",
    "                f\"{len(val_shards)} shards de validación\")\n",
    "\n",
    "    # Primer shard para entrenar scaler y dimensionar la red\n",
    "    df0 = pd.read_parquet(train_shards[0])\n",
    "    X0, y0 = prepare_features_target(df0)\n",
    "    scaler = StandardScaler().fit(X0)\n",
    "    n_features = X0.shape[1]\n",
    "\n",
    "    # Datasets y loaders\n",
    "    train_ds = ParquetShardsDataset(train_shards, batch_size, scaler=None)\n",
    "    val_ds   = ParquetShardsDataset(val_shards,   batch_size, scaler=scaler)\n",
    "    train_loader = DataLoader(train_ds, batch_size=None)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=None)\n",
    "\n",
    "    # Modelo, optimizador y loss\n",
    "    model = PriceNet(n_features).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # --- Entrenamiento ---\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(Xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch}/{n_epochs} — Train MSE: {np.mean(train_losses):.4f}\"\n",
    "        )\n",
    "\n",
    "        # --- Validación ---\n",
    "        model.eval()\n",
    "        val_mae = []\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb, yb = Xb.to(device), yb.to(device)\n",
    "                preds = model(Xb)\n",
    "                # MAE en GPU\n",
    "                mae = torch.mean(torch.abs(preds - yb)).item()\n",
    "                val_mae.append(mae)\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch}/{n_epochs} — Val MAE: {np.mean(val_mae):.4f}\"\n",
    "        )\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "# Ejecución\n",
    "# model, scaler = train_with_shards(\n",
    "#     train_dir=\"data/train\",\n",
    "#     val_dir=\"data/val\",\n",
    "#     batch_size=2048,\n",
    "#     lr=1e-3,\n",
    "#     n_epochs=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31767a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 1) Parámetros\n",
    "ENGINE_URL = \"postgresql://user:pass@host:port/dbname\"\n",
    "SCHEMA = \"schema_name\"\n",
    "TABLE = \"table_name\"\n",
    "ID_COL = \"primary_key_col\"\n",
    "STRAT_COL = \"column_to_stratify\"\n",
    "OUTPUT_DIR = \"data\"           # aquí crearás data/train y data/val\n",
    "BATCH_SIZE = 100_000          # filas por chunk al leer la BD\n",
    "SHARD_SIZE = 200_000          # filas por archivo Parquet\n",
    "\n",
    "# 2) Conectar a la base de datos\n",
    "engine = create_engine(ENGINE_URL)\n",
    "\n",
    "# 3) Leer sólo id y columna de estratificación\n",
    "query = f\"\"\"\n",
    "SELECT {ID_COL}, {STRAT_COL}\n",
    "FROM {SCHEMA}.{TABLE}\n",
    "\"\"\"\n",
    "df_index = pd.read_sql(query, engine)\n",
    "\n",
    "# 4) Split estratificado\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(\n",
    "    sss.split(df_index, df_index[STRAT_COL])\n",
    ")\n",
    "train_ids = set(df_index.iloc[train_idx][ID_COL])\n",
    "val_ids   = set(df_index.iloc[val_idx][ID_COL])\n",
    "del df_index  # liberar memoria\n",
    "\n",
    "# 5) Función para volcar shards Parquet\n",
    "def dump_shards(id_set, out_subdir):\n",
    "    os.makedirs(out_subdir, exist_ok=True)\n",
    "    shard_rows = []\n",
    "    shard_num = 0\n",
    "    offset = 0\n",
    "    \n",
    "    # Leer en chunks y filtrar por id_set\n",
    "    while True:\n",
    "        chunk = pd.read_sql(\n",
    "            f\"SELECT * FROM {SCHEMA}.{TABLE} \"\n",
    "            f\"LIMIT {BATCH_SIZE} OFFSET {offset}\",\n",
    "            engine\n",
    "        )\n",
    "        if chunk.empty:\n",
    "            break\n",
    "        offset += BATCH_SIZE\n",
    "        \n",
    "        # Filtrar\n",
    "        filtered = chunk[chunk[ID_COL].isin(id_set)]\n",
    "        for start in range(0, len(filtered), SHARD_SIZE):\n",
    "            shard = filtered.iloc[start:start+SHARD_SIZE]\n",
    "            path = os.path.join(out_subdir, f\"shard_{shard_num:03d}.parquet\")\n",
    "            shard.to_parquet(path, index=False)\n",
    "            print(f\"Escrito {path} ({len(shard)} filas)\")\n",
    "            shard_num += 1\n",
    "\n",
    "# 6) Volcar train y validation\n",
    "dump_shards(train_ids, os.path.join(OUTPUT_DIR, \"train\"))\n",
    "dump_shards(val_ids,   os.path.join(OUTPUT_DIR, \"val\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
